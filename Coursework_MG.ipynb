{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Coursework_MG.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MiroGasparek/DeepLearningCourse/blob/master/Coursework_MG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "CZdAvyqDk_rG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Coursework\n",
        "Based on the baseline code provided by the course instructors. The code introduces a two-step training for the N-HPatches problem, in which we aim to generate a patch descriptor that is able to perform successfully tasks such as matching, retrieval or verification.\n",
        "\n",
        "Contrary to classical HPatches dataset, in N-HPatches, images contain random non-smooth perturbations produced by a synthetic noise. This noise could be critical when training the descriptor, therefore, we introduce a denoising model that could help us to deal with those perturbations.\n",
        "\n",
        "Thus, we aim to minimize the noise in images before the second step, which is computing a feature vector, also called descriptor. Those descriptions must be a powerful representation of the input patches. The idea behind is that if two descriptors belong two similar patches, they should be close to each other, i.e. have a low Euclidean distance.\n",
        "\n",
        "This baseline code gives a method you can use to compare to whatever another approach you develop. There are several other approaches you can test to see if there is any improvement, e.g. train the descriptor directly with noisy patches, without the denoising model.\n",
        "\n",
        "## Safety checks\n",
        "\n",
        "As Google Colab is an external platform, we cannot guarantee that everytime you connect to a remote server, you will have the same amount of RAM or video RAM. For that reason, we will first check the amount of memory we have in the notebook. RAM should be around 12.9 GB, which is enough to load the datasets in memory. Also, usually, we have available 11.4 GB of GPU memory, which is more than enough to run this code.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "UZLPUD3JlZOj",
        "colab_type": "code",
        "outputId": "123a6e95-169a-41a8-fc76-ecd2861ab529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "cell_type": "code",
      "source": [
        "# Taken from\n",
        "# https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# Colab only provides one GPU and it is not always guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5ce6bfbdb6ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mGPUs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetGPUs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Colab only provides one GPU and it is not always guaranteed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mgpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPUs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprintm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "hYETGP-1mfGY",
        "colab_type": "code",
        "outputId": "055b0363-4e1a-41d7-b507-9b511c3f7b93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "printm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-abcb48d0db41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprintm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'printm' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "XkQ7gXSknn6g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Downloading Functions and Data\n",
        "The first step is to clone the GitHub repository of the course, which contains already implemented functions. You can use your own function and import them here doing the same. In addition, we are going to download and extract the N-HPatches data.\n",
        "\n",
        "As a note, in colab, we can run terminal commands by using `!`. Also, by using `%` we have access to the built-in IPython magic commands, which we will use to move through directories (cd command). It takes around 5 minutes to download and unzip the dataset."
      ]
    },
    {
      "metadata": {
        "id": "lWbswTo_nxdI",
        "colab_type": "code",
        "outputId": "4ea06d34-782a-4d0b-b111-af5705bf85e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "# Clone repo\n",
        "!git clone https://github.com/MatchLab-Imperial/keras_triplet_descriptor"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras_triplet_descriptor'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 172 (delta 21), reused 22 (delta 10), pack-reused 128\u001b[K\n",
            "Receiving objects: 100% (172/172), 149.84 MiB | 21.77 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n",
            "Checking out files: 100% (69/69), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zWJ049SNn7pW",
        "colab_type": "code",
        "outputId": "cf758ff1-c0a7-471c-ac97-85f179c91aec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Change directory\n",
        "%cd /content/keras_triplet_descriptor"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/keras_triplet_descriptor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2-uLEr5QoCGY",
        "colab_type": "code",
        "outputId": "058924fe-931b-45cd-c348-58a90432223d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        }
      },
      "cell_type": "code",
      "source": [
        "# Download data\n",
        "!wget -O hpatches_data.zip https://imperialcollegelondon.box.com/shared/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-20 05:36:00--  https://imperialcollegelondon.box.com/shared/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n",
            "Resolving imperialcollegelondon.box.com (imperialcollegelondon.box.com)... 107.152.27.197, 107.152.26.197\n",
            "Connecting to imperialcollegelondon.box.com (imperialcollegelondon.box.com)|107.152.27.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip [following]\n",
            "--2019-02-20 05:36:01--  https://imperialcollegelondon.box.com/public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n",
            "Reusing existing connection to imperialcollegelondon.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://imperialcollegelondon.app.box.com/public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip [following]\n",
            "--2019-02-20 05:36:01--  https://imperialcollegelondon.app.box.com/public/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n",
            "Resolving imperialcollegelondon.app.box.com (imperialcollegelondon.app.box.com)... 107.152.27.199\n",
            "Connecting to imperialcollegelondon.app.box.com (imperialcollegelondon.app.box.com)|107.152.27.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!6FreVTXb1NVfw0xrLIovx9tz6AHEWu3Q4djWYpDMx7YzMTTvvzwevRT1Jnhop2Ydm_mpMCZ5nWgjO1_RZXoYIT_YA_e49CUJ6uHpzJwKcn54K6ulw53_XfN9ANY9BmvOyuG7lscwFklUNDADJQL4ozgTviG5YqrFVNArnlceDA6usfTyXtFtHBk5qhG7QNv3oV1pjclr0iEvav28UiqpQz-ichRhxWeIv0YMi6Ty7QGx7yy9Q1iKeeylPmVpO3L7LRgeNwnXM-E_vyhTbPajRx8VqVW_dqP_ffnaI1NY152ZOgf8FNvAURkJh46mHuxfP82sR34yL9I0yjiQbWoI552_ZjaA_tUoc_D5NyEZbwOrDBXWUQ_iLVjM5WBZtfeYcIANyuUav3QkYuUEOCjGmVeG0021FIGvHedQrPNsCMUkWBJjgQwsa5mk5FG2_Qyl-eWs6zuj6An_eZpLtvLqIZQH1fUIlvDfANPWY4KqXdAbrUCa50Y1mncD640kvfKx3Jo9hG0uJ47VIpZzwITMLgunJT_Du61s6mlHpohyL0cRE9AcQNPajN5YYzviNFndXRgoEfHyZho5LAXFqs9xM9UdSrfTH54eXz-gxhOSEPkb6VbjSN_CZ7lcZXpPRnysn3l0KEqBcPaL8koqSJsnd75N1HyF0yZ-CdVuM3mCYRywwPFdh83fNb8bJPWldPNcfDG0pN6oAGDTmmI1u9qWRH16AUUuFU7EUPjYKpcpL4YhOiPHMQrh7LRUUlm8JZ9MsESywznK5lrerp9Y1mf9i_2pAuxmSzwZU9Sr7CK6p8vk_xozugMbGLRuEw1Unre8Im8dAobGpZsWlYos40iPXnVjYaL2yt_OYuLqmHybn8F_Ax2XYZDh51Nd-VZV_TKiVdy887tzv6MPGiX5Yw_S8y6zLa57Z16ssrTia-IXUQGoHgyGOhl2VjA-kzYZC2bEhaqcKB9Q4V5dOBipD_JLq1n_0o4STaHVymVgAm0XEwQUZJ5fhASJ0K9XgF_Qan8RjqnvI-z9QtoO4GHFc1_QrrSO3K0DgO33qXnaT-cRnV7nnLgJ330Ax7wRLL-Q6cBk-Oa5JaomopIgUUxlgRVJMc0pUeKSbKCXLx2CZjRPoghwtuWgkGyqZj71y4Sm4rERqvIaSI3JWyIvu9cKwchkS2c3CHu0lsA_CbVUQSbcMf0EQirL24FPkLg7KLpusnMC4v4z96siMfWHRO3Y_YFVuU6frrDIQAIxoTP4xeEqDYam7TdRtrspsl1k1Dr9gZZPXz-IkrN6C-r2HyJs2rz-dNsQqmQu3XFPugk8Wx_1RvhD_tXLqNrmZDaH3M1_UlU_0k7A_i-Fo3-fCJwU0STIKn7pY7LwI_5NMvZilXWi-Rdl8BcsXn2EJL6pozwoG42ckLHAtLb3pysIt5aQY2wH_w49fNP3nPhiKoe6_HG5WJLgZEg./download [following]\n",
            "--2019-02-20 05:36:01--  https://public.boxcloud.com/d/1/b1!6FreVTXb1NVfw0xrLIovx9tz6AHEWu3Q4djWYpDMx7YzMTTvvzwevRT1Jnhop2Ydm_mpMCZ5nWgjO1_RZXoYIT_YA_e49CUJ6uHpzJwKcn54K6ulw53_XfN9ANY9BmvOyuG7lscwFklUNDADJQL4ozgTviG5YqrFVNArnlceDA6usfTyXtFtHBk5qhG7QNv3oV1pjclr0iEvav28UiqpQz-ichRhxWeIv0YMi6Ty7QGx7yy9Q1iKeeylPmVpO3L7LRgeNwnXM-E_vyhTbPajRx8VqVW_dqP_ffnaI1NY152ZOgf8FNvAURkJh46mHuxfP82sR34yL9I0yjiQbWoI552_ZjaA_tUoc_D5NyEZbwOrDBXWUQ_iLVjM5WBZtfeYcIANyuUav3QkYuUEOCjGmVeG0021FIGvHedQrPNsCMUkWBJjgQwsa5mk5FG2_Qyl-eWs6zuj6An_eZpLtvLqIZQH1fUIlvDfANPWY4KqXdAbrUCa50Y1mncD640kvfKx3Jo9hG0uJ47VIpZzwITMLgunJT_Du61s6mlHpohyL0cRE9AcQNPajN5YYzviNFndXRgoEfHyZho5LAXFqs9xM9UdSrfTH54eXz-gxhOSEPkb6VbjSN_CZ7lcZXpPRnysn3l0KEqBcPaL8koqSJsnd75N1HyF0yZ-CdVuM3mCYRywwPFdh83fNb8bJPWldPNcfDG0pN6oAGDTmmI1u9qWRH16AUUuFU7EUPjYKpcpL4YhOiPHMQrh7LRUUlm8JZ9MsESywznK5lrerp9Y1mf9i_2pAuxmSzwZU9Sr7CK6p8vk_xozugMbGLRuEw1Unre8Im8dAobGpZsWlYos40iPXnVjYaL2yt_OYuLqmHybn8F_Ax2XYZDh51Nd-VZV_TKiVdy887tzv6MPGiX5Yw_S8y6zLa57Z16ssrTia-IXUQGoHgyGOhl2VjA-kzYZC2bEhaqcKB9Q4V5dOBipD_JLq1n_0o4STaHVymVgAm0XEwQUZJ5fhASJ0K9XgF_Qan8RjqnvI-z9QtoO4GHFc1_QrrSO3K0DgO33qXnaT-cRnV7nnLgJ330Ax7wRLL-Q6cBk-Oa5JaomopIgUUxlgRVJMc0pUeKSbKCXLx2CZjRPoghwtuWgkGyqZj71y4Sm4rERqvIaSI3JWyIvu9cKwchkS2c3CHu0lsA_CbVUQSbcMf0EQirL24FPkLg7KLpusnMC4v4z96siMfWHRO3Y_YFVuU6frrDIQAIxoTP4xeEqDYam7TdRtrspsl1k1Dr9gZZPXz-IkrN6C-r2HyJs2rz-dNsQqmQu3XFPugk8Wx_1RvhD_tXLqNrmZDaH3M1_UlU_0k7A_i-Fo3-fCJwU0STIKn7pY7LwI_5NMvZilXWi-Rdl8BcsXn2EJL6pozwoG42ckLHAtLb3pysIt5aQY2wH_w49fNP3nPhiKoe6_HG5WJLgZEg./download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 107.152.27.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|107.152.27.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4088106554 (3.8G) [application/zip]\n",
            "Saving to: ‘hpatches_data.zip’\n",
            "\n",
            "hpatches_data.zip   100%[===================>]   3.81G  21.3MB/s    in 3m 3s   \n",
            "\n",
            "2019-02-20 05:39:05 (21.3 MB/s) - ‘hpatches_data.zip’ saved [4088106554/4088106554]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uac4VJgLq1nU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extract data\n",
        "!unzip -q ./hpatches_data.zip\n",
        "!rm ./hpatches_data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cur7xb3KrVNZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Importing Necessary Modules\n",
        "We now import the modules we will use in this baseline code."
      ]
    },
    {
      "metadata": {
        "id": "0PbKrRb5oFUD",
        "colab_type": "code",
        "outputId": "0241adb7-8367-4644-ebc8-8b0f82c4d26a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization \n",
        "from keras.layers import Input, UpSampling2D, concatenate  \n",
        "\n",
        "from read_data import HPatches, DataGeneratorDesc, hpatches_sequence_folder, DenoiseHPatches, tps\n",
        "from utils import generate_desc_csv, plot_denoise, plot_triplet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "pk6xhMuNrwNK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Fix the seeds of the pseudo-random number generators to have reproducible results. The idea of fixing the seed is having the same results every time the algorithm is run if there are no changes in the code."
      ]
    },
    {
      "metadata": {
        "id": "-vGq5Urorbm8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# fix random numbers generator\n",
        "random.seed(1234)\n",
        "np.random.seed(1234)\n",
        "tf.set_random_seed(1234)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DgSHLP0JsE2g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we load the data. The original HPatches dataset has several splits, which are used to separate the available sequences in train sequences and test sequences. For our experiments in N-HPatches we use the same splits as in HPatches. Specifically, we load (and report results) using the split 'a':"
      ]
    },
    {
      "metadata": {
        "id": "tzw9KBQYsApi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hpatches_dir = './hpatches'\n",
        "splits_path = './splits.json'\n",
        "\n",
        "splits_json = json.load(open(splits_path, 'rb'))\n",
        "split = splits_json['a']\n",
        "\n",
        "train_fnames = split['train']\n",
        "test_fnames = split['test']\n",
        "\n",
        "seqs = glob.glob(hpatches_dir+'/*')\n",
        "seqs = [os.path.abspath(p) for p in seqs]   \n",
        "seqs_train = list(filter(lambda x: x.split('/')[-1] in train_fnames, seqs)) \n",
        "seqs_test = list(filter(lambda x: x.split('/')[-1] in split['test'], seqs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bEd2RRyOaVbL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Models and loss\n",
        "Below are the three functions that define the main modules of the baseline approach:\n",
        "* `get_denoise_model(..)` returns the denoising model. The input of this function is the size of the patch, which is 1x32x32 and it outputs a keras denoising model.\n",
        "* `get_descriptor_model(..)` builds the descriptor model. The input for the function is the size of the patch, which is 1x32x32 and it outputs a keras descriptor model. The model we use as baseline returns a descriptor of dimension 128x1.\n",
        "* `triplet_loss(..)` defines the loss function for the training of the descriptor model."
      ]
    },
    {
      "metadata": {
        "id": "jtJUiKYDsIrL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_denoise_model(shape):\n",
        "    \n",
        "  inputs = Input(shape)\n",
        "  \n",
        "  ## Encoder starts\n",
        "  conv1 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "  pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "  \n",
        "  ## Bottleneck\n",
        "  conv2 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "\n",
        "  ## Now the decoder starts\n",
        "  up3 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv2))\n",
        "  merge3 = concatenate([conv1,up3], axis = -1)\n",
        "  conv3 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge3)\n",
        "    \n",
        "  conv4 = Conv2D(1, 3,  padding = 'same')(conv3)\n",
        "\n",
        "  shallow_net = Model(inputs = inputs, outputs = conv4)\n",
        "  \n",
        "  return shallow_net\n",
        "\n",
        "\n",
        "\n",
        "def get_descriptor_model(shape):\n",
        "  \n",
        "  '''Architecture copies HardNet architecture'''\n",
        "  \n",
        "  init_weights = keras.initializers.he_normal()\n",
        "  \n",
        "  descriptor_model = Sequential()\n",
        "  descriptor_model.add(Conv2D(32, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(32, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(64, 3, padding='same', strides=2, use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(64, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(128, 3, padding='same', strides=2,  use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(128, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "  descriptor_model.add(Dropout(0.3))\n",
        "\n",
        "  descriptor_model.add(Conv2D(128, 8, padding='valid', use_bias = True, kernel_initializer=init_weights))\n",
        "  \n",
        "  # Final descriptor reshape\n",
        "  descriptor_model.add(Reshape((128,)))\n",
        "  \n",
        "  return descriptor_model\n",
        "  \n",
        "  \n",
        "def triplet_loss(x):\n",
        "  \n",
        "  output_dim = 128\n",
        "  a, p, n = x\n",
        "  _alpha = 1.0\n",
        "  positive_distance = K.mean(K.square(a - p), axis=-1)\n",
        "  negative_distance = K.mean(K.square(a - n), axis=-1)\n",
        "  \n",
        "  return K.expand_dims(K.maximum(0.0, positive_distance - negative_distance + _alpha), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3vWkup84dWNH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Denoising image patches\n",
        "We use the DenoiseHPatches class implemented in the read_data.py file, which takes as input the list of sequences to load and the size of batches.\n",
        "\n",
        "DenoiseHPatches outputs batches where the input data is the noisy image and the label is the clean image, so we can use a mean absolute error (MSE) metric as loss function. You can try to use different metrics here to see if that improves results.\n",
        "\n",
        "Afterward, we take a subset of training and validation sequences by using random.sample (3 sequences for training and 1 for validation data). The purpose of doing so is just to speed-up training when trying different setups, but you should use the whole dataset when training your final model. Remove the random.sample function to give the generator all the training data.\n",
        "\n",
        "In addition, note that we are using the test set as validation."
      ]
    },
    {
      "metadata": {
        "id": "0PnMCfGVdTFo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "cfd47618-b2d2-426d-c068-e53936ba4cfb"
      },
      "cell_type": "code",
      "source": [
        "denoise_generator = DenoiseHPatches(random.sample(seqs_train, 3), batch_size=50)\n",
        "denoise_generator_val = DenoiseHPatches(random.sample(seqs_test, 1), batch_size=50)\n",
        "\n",
        "# Uncomment following lines for using all the data to train the denoising model\n",
        "# denoise_generator = DenoiseHPatches(seqs_train, batch_size=50)\n",
        "# denoise_generator_val = DenoiseHPatches(seqs_test, batch_size=50)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-42acce6abdad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdenoise_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDenoiseHPatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdenoise_generator_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDenoiseHPatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Uncomment following lines for using all the data to train the denoising model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# denoise_generator = DenoiseHPatches(seqs_train, batch_size=50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DenoiseHPatches' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "H5nr-BJAdlw4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}